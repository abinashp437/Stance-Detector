{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transormer_fnc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjTZVTi6UWTDBURqTRlVdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abinashp437/Stance_Detection_FNC_1/blob/main/transormer_fnc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UmDW7oZ1i9_",
        "outputId": "b2096d02-d259-4ff8-85f9-12a8111957f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords') #for stopword removal\n",
        "nltk.download('wordnet') #for lemmatisation\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQlrw8Js5tEp"
      },
      "source": [
        "body_url = \"https://raw.githubusercontent.com/abinashp437/Stance_Detection_FNC_1/main/fnc_1_data/train_bodies.csv\"\n",
        "body = pd.read_csv(body_url)\n",
        "stance_url = \"https://raw.githubusercontent.com/abinashp437/Stance_Detection_FNC_1/main/fnc_1_data/train_stances.csv\"\n",
        "stance = pd.read_csv(stance_url)\n",
        "stance.sort_values(['Body ID'], axis = 0, ascending = True,  inplace = True, ignore_index = True)\n",
        "count = stance['Body ID'].value_counts(sort = False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfQzuacW6BLY",
        "outputId": "75b41c40-23e9-4a81-ee2b-9611b88d7c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for row_id, art in body.iterrows():\n",
        "  body['articleBody'][row_id] = art['articleBody'].replace('\\n','')\n",
        "for row_id, stan in stance.iterrows():\n",
        "  stance['Headline'][row_id] = stan['Headline'].replace('\\n','')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMqtBPG16b79",
        "outputId": "ee349f4a-3bbc-41ad-b8f3-30d1fd129682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "for row_id, art in body.iterrows():\n",
        "  body['articleBody'][row_id] = ' '.join([word for word in art['articleBody'].split() if word not in stop_words])\n",
        "for row_id, head in stance.iterrows():\n",
        "  stance['Headline'][row_id] = ' '.join([word for word in head['Headline'].split() if word not in stop_words])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DarDlVmL6foa",
        "outputId": "348eb980-80a5-4c10-ecd4-9476feb9b7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize(text):\n",
        "  return [lemmatizer.lemmatize(word) for word in w_tokenizer.tokenize(text)]\n",
        "\n",
        "for row_id, art in body.iterrows():\n",
        "  body['articleBody'][row_id] = ' '.join(lemmatize(art['articleBody']))\n",
        "\n",
        "for row_id, head in stance.iterrows():\n",
        "  stance['Headline'][row_id] = ' '.join(lemmatize(head['Headline']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2PORhTf6pus",
        "outputId": "fc495e0e-e8cc-44e4-8222-609200e8a299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for row_id, art in body.iterrows():\n",
        "  body['articleBody'][row_id] = art['articleBody'].lower()\n",
        "\n",
        "for row_id, head in stance.iterrows():\n",
        "  stance['Headline'][row_id] = head['Headline'].lower()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GYcGSP46vnZ"
      },
      "source": [
        "corpus = []\n",
        "for row_id, art in body.iterrows():\n",
        "  corpus.append(w_tokenizer.tokenize(art['articleBody']))\n",
        "\n",
        "for row_id, head in stance.iterrows():\n",
        "  corpus.append(w_tokenizer.tokenize(head['Headline']))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8p6L5aN6-zG",
        "outputId": "98a179af-63b1-4829-e42f-2ab0b9ea4e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pad_art = 180\n",
        "pad_head = 11\n",
        "for row_id, art in body.iterrows():\n",
        "  body['articleBody'][row_id] = ' '.join(w_tokenizer.tokenize(art['articleBody'])[:pad_art])\n",
        "\n",
        "for row_id, head in stance.iterrows():\n",
        "  stance['Headline'][row_id] = ' '.join(w_tokenizer.tokenize(head['Headline'])[:pad_head])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_4E8tqM7DPj",
        "outputId": "04d4d9b8-8a36-44bf-c0d9-46512012acaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Word2Vec(corpus, min_count = 1, size = 100, workers = 3, window = 3, iter = 30)\n",
        "def vectorize(text):\n",
        "  return [model[word] for word in w_tokenizer.tokenize(text)]\n",
        "\n",
        "art_vec = pd.Series([], dtype = float)\n",
        "for row_id, art in body.iterrows():\n",
        "  art_vec[row_id] = vectorize(art['articleBody'])\n",
        "\n",
        "head_vec = pd.Series([], dtype = float)\n",
        "for row_id, head in stance.iterrows():\n",
        "  head_vec[row_id] = vectorize(head['Headline'])\n",
        "\n",
        "pad_art_vec = keras.preprocessing.sequence.pad_sequences(art_vec, padding = 'post', maxlen = pad_art, dtype = float)\n",
        "pad_head_vec = keras.preprocessing.sequence.pad_sequences(head_vec, padding = 'post', maxlen = pad_head, dtype = float)\n",
        "pad_art_vec = pad_art_vec.tolist()\n",
        "pad_head_vec = pad_head_vec.tolist()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr05QQbV7fva"
      },
      "source": [
        "article_vector = pd.DataFrame()\n",
        "article_vector.insert(0, 'Body ID', body['Body ID'])\n",
        "article_vector.insert(1, 'art_Vectors', pad_art_vec)\n",
        "\n",
        "headline_vector = pd.DataFrame()\n",
        "headline_vector.insert(0, 'Body ID', stance['Body ID'])\n",
        "headline_vector.insert(1, 'head_Vectors', pad_head_vec)\n",
        "\n",
        "vectors = pd.merge(article_vector, headline_vector, left_on = 'Body ID', right_on = 'Body ID')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS0jA2Hg_GI-"
      },
      "source": [
        "**Training and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52BpkYdR8sm-"
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional, Concatenate, concatenate, Dense, Input, LSTM\n",
        "from keras.models import Model\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q8MinBw_fh0"
      },
      "source": [
        "Stan_Id = pd.Series([], dtype = int)\n",
        "for row_id, stan in stance.iterrows():\n",
        "  if stan['Stance'] == 'unrelated':\n",
        "    Stan_Id[row_id] = 0\n",
        "  elif stan['Stance'] == 'agree':\n",
        "    Stan_Id[row_id] = 1\n",
        "  elif stan['Stance'] == 'disagree':\n",
        "    Stan_Id[row_id] = 2\n",
        "  elif stan['Stance'] == 'discuss':\n",
        "    Stan_Id[row_id] = 3\n",
        "\n",
        "vectors.insert(3, 'Stance ID', Stan_Id)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zlxke9WQOmw"
      },
      "source": [
        "def data(vector):\n",
        "  x = pd.DataFrame()\n",
        "  y = pd.DataFrame()\n",
        "  x.insert(0, 'head', vector['head_Vectors'])\n",
        "  x.insert(1, 'art', vector['art_Vectors'])\n",
        "  y.insert(0, 'Stan_ID', vector['Stance ID'])\n",
        "  return x, y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dpTkOLFM9Dv"
      },
      "source": [
        "def train_lstm(head_states, art_states, pad_art, pad_head, art_train, head_train, stance_train):\n",
        "  x = Input(shape = (pad_head, 100))\n",
        "  y = Input(shape = (pad_art, 100))\n",
        "  head, forwardh_h, forwardh_c, backwardh_h, backwardh_c = Bidirectional(LSTM(50, dropout = 0.1, return_sequences = False, return_state = True))(x, initial_state = head_states)\n",
        "  print(head)\n",
        "  art, forwarda_h, forwarda_c, backwarda_h, backwarda_c = Bidirectional(LSTM(50, dropout = 0.1, return_sequences = False, return_state = True))(y, initial_state = art_states)\n",
        "  print(art)\n",
        "  head_states = ([forwardh_h, forwardh_c, backwardh_h, backwardh_c])\n",
        "  art_states = ([forwarda_h, forwarda_c, backwarda_h, backwarda_c])\n",
        "  print(forwardh_h.shape, forwarda_h.shape)\n",
        "  tot = concatenate([head, art], axis = -1)\n",
        "  tot = Dense(16, activation='sigmoid')(tot)\n",
        "  total = Dense(5,activation='softmax')(tot)\n",
        "  model_lstm = Model(inputs = [x, y], outputs = total)\n",
        "  model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  print(head_train.shape, art_train.shape)\n",
        "\n",
        "  fitmodel = model_lstm.fit([head_train, art_train], stance_train, epochs = 20, verbose = 1)\n",
        "  return fitmodel, fitmodel.history['accuracy'], head_states, art_states"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duHa7BEZSOs_",
        "outputId": "1c898c41-9d96-4a32-ea9b-728708fc8f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = 0\n",
        "head_states = ([tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32)), tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32)), tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32)), tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32))])\n",
        "art_states = ([tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32)), tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32)), tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32)), tf.convert_to_tensor(np.random.random([50,50]).astype(np.float32))])\n",
        "global_accuracy_list = []\n",
        "for row_id, row in article_vector.iterrows():\n",
        "  end = start + count[row['Body ID']]\n",
        "  final_vector = vectors.iloc[start:end]\n",
        "  start = end\n",
        "  x_train, y_train = data(final_vector)\n",
        "  head_train = tf.convert_to_tensor(x_train['head'].tolist())\n",
        "  art_train = tf.convert_to_tensor(x_train['art'].tolist())\n",
        "  stance_train = tf.convert_to_tensor(y_train)\n",
        "  print('\\n\\n\\n', start, end, head_train.shape, art_train.shape)\n",
        "  mod, accuracy, head_states, art_states = train_lstm(head_states, art_states, pad_art, pad_head, art_train, head_train, stance_train)\n",
        "  print(mod)\n",
        "  print('\\n\\n\\n')\n",
        "  global_accuracy_list.append(accuracy)\n",
        "\n",
        "print(end)\n",
        "mod.save('model_lstm.h5')\n",
        "files.download('model_lstm.h5')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 36 36 (36, 11, 100) (36, 180, 100)\n",
            "Tensor(\"bidirectional_18/concat:0\", shape=(50, 100), dtype=float32)\n",
            "Tensor(\"bidirectional_19/concat:0\", shape=(50, 100), dtype=float32)\n",
            "(50, 50) (50, 50)\n",
            "(36, 11, 100) (36, 180, 100)\n",
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-314e973a1624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mstance_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_art\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstance_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-6dd294e510e0>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(head_states, art_states, pad_art, pad_head, art_train, head_train, stance_train)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mfitmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstance_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfitmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mart_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:    Invalid input_h shape: [1,50,50] [1,32,50]\n\t [[{{node CudnnRNN}}]]\n\t [[functional_15/bidirectional_18/backward_lstm_18/PartitionedCall]] [Op:__inference_train_function_79123]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssv_VQZ3pWsF"
      },
      "source": [
        "y = np.average(global_accuracy_list, axis = 0)\n",
        "\n",
        "plt.plot(y, color = 'r')\n",
        "plt.xticks(ticks=np.arange(0,20), labels=np.arange(1,21))\n",
        "plt.yticks(ticks=np.arange(0,10), labels=[round( _ * 0.1, 1) for _ in range(1,11))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV_9-DegeAkZ"
      },
      "source": [
        "# x = Input(shape = (pad_head, 100))\n",
        "# y = Input(shape = (pad_art, 100))\n",
        "# head = Bidirectional(LSTM(50, dropout = 0.5, return_sequences = False))(x)\n",
        "# art = Bidirectional(LSTM(50, dropout = 0.5, return_sequences = False))(y)\n",
        "# tot = concatenate([head, art], axis = -1)\n",
        "# tot = Dense(16, activation='sigmoid')(tot)\n",
        "# total = Dense(5,activation='softmax')(tot)\n",
        "# model_lstm = Model(inputs = [x, y], outputs = total)\n",
        "# model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fitmodel = model_lstm.fit([head_train, art_train], stance_train, epochs = 20, verbose = 1, shuffle=False)\n",
        "\n",
        "# model_lstm.save('model_lstm.h5')\n",
        "# files.download('model_lstm.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}